# LLM Bias Benchmarking & Mitigation

Benchmarking and evaluating bias in large language models (LLMs) using **Jupyter Notebooks**.  
This project explores model behavior across bias-sensitive datasets (e.g., HolisticBias, CrowS-Pairs, StereoSet) and evaluates mitigation strategies.  

The repository is designed for **collaborative experimentation**: notebooks handle the core workflow, while optional helper modules keep code reusable and clean. Future extensions (e.g., a dashboard or API) can plug in without restructuring.

---

## ğŸš€ Goals

- **Benchmark** multiple LLMs (DeepSeek-R1, Llama-3.1, Qwen-3, Gemma-3)  
- **Audit bias** using standard datasets (HolisticBias, CrowS-Pairs, StereoSet)  
- **Simulate attacks** (e.g., social engineering, prompt injection)  
- **Evaluate mitigation strategies** and record their effect on metrics  
- **Aggregate results** into structured exports for reporting or dashboards  

---

## ğŸ“‚ Repository Layout

ğŸ“¦ llm-bias-benchmark
â”£ ğŸ“’ notebooks/ # Jupyter notebooks for experiments
â”ƒ â”£ ğŸ“˜ 00_setup.ipynb # environment checks, imports, API keys
â”ƒ â”£ ğŸ“˜ 10_datasets.ipynb # dataset loading & preprocessing
â”ƒ â”£ ğŸ“˜ 20_baseline_eval.ipynb # initial bias evaluation on models
â”ƒ â”£ ğŸ“˜ 30_mitigation_eval.ipynb # test mitigation strategies
â”ƒ â”£ ğŸ“˜ 40_stats_and_plots.ipynb # aggregate metrics, generate charts
â”ƒ â”— ğŸ“˜ 90_export_results.ipynb # prepare data for reporting/dashboard
â”£ ğŸ“‚ data/ # (gitignored) datasets
â”ƒ â”£ ğŸ“‚ raw/ # original downloads
â”ƒ â”£ ğŸ“‚ interim/ # cleaned/intermediate data
â”ƒ â”— ğŸ“‚ processed/ # ready-to-use datasets
â”£ ğŸ“‚ results/ # (gitignored) outputs
â”ƒ â”£ ğŸ“‚ runs/ # per-run artifacts (metrics, logs, plots)
â”ƒ â”— ğŸ“‚ exports/ # aggregated results for dashboards/reports
â”£ ğŸ“‚ src/ # optional Python helpers
â”ƒ â”— ğŸ“‚ lbm/
â”ƒ â”£ ğŸ“„ datasets.py # dataset loaders
â”ƒ â”£ ğŸ“„ attacks.py # attack templates
â”ƒ â”£ ğŸ“„ models.py # model API wrappers
â”ƒ â”£ ğŸ“„ metrics.py # bias/fairness metrics
â”ƒ â”— ğŸ“„ eval.py # evaluation loops
â”£ ğŸ“‚ tests/ # unit tests for metrics/helpers
â”£ ğŸ“‚ dashboard/ (optional) # placeholder for future visualisation
â”£ ğŸ“„ .gitignore
â”£ ğŸ“„ README.md
â”— ğŸ“„ requirements.txt or pyproject.toml

---

## ğŸ“Š Workflow

1. **Setup notebook** (`00_setup.ipynb`) â†’ check dependencies & keys  
2. **Load datasets** (`10_datasets.ipynb`) â†’ preprocess bias benchmarks  
3. **Baseline evaluation** (`20_baseline_eval.ipynb`) â†’ test raw model behavior  
4. **Apply attacks & mitigations** (`30_mitigation_eval.ipynb`)  
5. **Aggregate metrics & plots** (`40_stats_and_plots.ipynb`)  
6. **Export results** (`90_export_results.ipynb`) â†’ JSON/CSV for reporting/dashboard  

---

## ğŸ“ˆ Optional/Future Extensions

- **Dashboard** (React/Chakra UI, Next.js, or Streamlit) to visualise results interactively  
- **API layer** (FastAPI or Flask) to serve metrics to external tools  
- **Additional datasets** for fairness and robustness evaluation  
- **Expanded attacks** (more prompt-injection and social engineering templates)  

---

## ğŸ“ License
MIT
